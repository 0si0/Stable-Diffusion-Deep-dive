{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Stable Diffusion Deep Dive"
      ],
      "metadata": {
        "id": "2IAC0CS2BBY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "원래 Stable Diffusion은 간단하게 Pipeline 을 활용하여\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision = \"fp16\", torch_dtype = torch.float16, use_auth_token = True).to(\"cuda\")\n",
        "\n",
        "image = pipe(\"An astronaut scuba Diving\").images[0]\n",
        "\n",
        "이렇게 간단히 할 수 있지만, 그 속의 세부적인 작동을 직접 코드로 구현해보았다."
      ],
      "metadata": {
        "id": "JqvmyNhCBmEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --update transformers diffusers ftfy\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NQmDreN-BkoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stable Diffusion을 구성하는 요소들을 잘 살펴보자\n",
        "\n",
        "prompt를 토큰으로 각각 처리하기 위해서 transformer에서 CLIP 2개를 불러왔고,\n",
        "\n",
        "VAE = latent를 실제 이미지로 복원해주는 Autoencoder\n",
        "\n",
        "Unet도 필요하고, LMSDiscreteScheduler는 timestep마다 update되는 latent를 관리한다.\n"
      ],
      "metadata": {
        "id": "wxTnCVhOK1j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "from transformers import logging # 모델 로딩할 때 경고/정보 메시지가 엄청 많이 뜨는 걸 조절\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
        "from tqdm.auto import tqdm # progressing bar 자동으로 골라줌\n",
        "from torch import autocast # autocast() 컨텍스트 안에서는 PyTorch가 “적절한 연산만 FP16으로” 자동 변환\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy\n",
        "from torchvision import transforms as tfms\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "torch_device = \"cuda\""
      ],
      "metadata": {
        "id": "J779F5hHHKK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder = \"vae\")\n",
        "\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder = \"unet\")\n",
        "\n",
        "# beta_start = 시작할 때 노이즈값(깨끗) beta_end = 학습 후 노이즈값\n",
        "scheduler = LMSDiscreteScheduler(beta_start =0.00085, beta_end = 0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "\n",
        "# 다 GPU로 ㄱㄱ\n",
        "vae = vae.to(torch_device)\n",
        "text_encoder = text_encoder.to(torch_device)\n",
        "unet = unet.to(torch_device)\n"
      ],
      "metadata": {
        "id": "ETbWh4EEJ6AR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A diffusion Loop"
      ],
      "metadata": {
        "id": "lcIK0LosnQg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\"A watercolor paintings of an cat\"]\n",
        "height = 512\n",
        "width = 512\n",
        "num_inference_steps = 30\n",
        "guidance_scale = 7.5\n",
        "generator = torch.manual_seed(32)\n",
        "batch_size = 1\n",
        "\n",
        "# Prepare Text\n",
        "# padding은 자릿수만큼 빈 자리 채우기  max_length를 모델이 기대하는 최대길이로 맞춤, truncation = 최대값 넘으면 잘라냄 True, 출력 텐서를 pytorch로 받음\n",
        "text_input = tokenizer(prompt, padding = \"max_length\", max_length = tokenizer.model_max_length, truncation = True, return_tensors = \"pt\")\n",
        "with torch.no_grad(): # 토큰 id 텐서를 cuda로 올리는 코드, [0]은 last_hidden_state (각 토큰의 임베딩)을 의미\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer([\"\"] * batch_size, padding = \"max_length\", return_tensors = \"pt\")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"
      ],
      "metadata": {
        "id": "XMkQ5XgbnUzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifier-Free Guidance를 위해서 conditioned embedding과 unconditioned embedding 이렇게 2개로 분리해서 생성해놓는다.\n",
        "\n",
        "CFG = 프롬프트가 없을 때의 결과와 비교해서 더 프롬프트를 세게 강조하자는 의도로 쓰인다.\n",
        "\n",
        "uncondition으로 돌린결과와 conditioned로 돌린결과를 뺀 만큼의 차이를 반영하여 더 프롬프트를 강하게 반영하는 쪽으로 진행한다."
      ],
      "metadata": {
        "id": "bF8ON9sItIEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Scheduler\n",
        "def set_timesteps(scheduler, num_inference_steps):\n",
        "    scheduler.set_timesteps(num_inference_steps)\n",
        "    scheduler.timesteps = scheduler.timesteps.to(torch.float32)\n",
        "\n",
        "set_timesteps(scheduler,num_inference_steps)\n",
        "\n",
        "\n",
        "# Prepare Latent\n",
        "latents = torch.randn(batch_size, unet.in_channels, height // 8, width // 8, generator = generator) # generator는 32번 시드로 고정\n",
        "latents = latents.to(torch_device) # unet이 input으로 latent받으므로 latent도 gpu에 올려놓기\n",
        "latents = latents * scheduler.init_noise_sigma # 초기 노이즈 스케일링"
      ],
      "metadata": {
        "id": "Z7UxDCrBttOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with autocast(\"cuda\"):\n",
        "  # i에는 loop index(0번째, 1번째, ..) t에는 현재 timestep\n",
        "  for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "    latent_model_input = torch.cat([latents] * 2) # CFG를 위해서 2배로\n",
        "    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      noise_pred = unet(latent_model_input, t, encoder_hidden_states = text_embeddings).sample\n",
        "\n",
        "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "    latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
        "\n",
        "latents = 1 / 0.18215 * latents\n",
        "with torch.no_grad():\n",
        "  image = vae.decode(latents).sample # vae가 latent를 image로 변환\n",
        "\n",
        "image = (image / 2 + 0.5).clamp(0, 1)\n",
        "# PyTorch 이미지 텐서는 [B, C, H, W] 일반 이미지 배열은 [B, H, W, C]가 더 흔함 그래서 축 순서 바꿈.\n",
        "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "images = (image * 255).round().astype(\"uint8\") # 이미지 라이브러리 표준 형태로 바꿈\n",
        "pil_images = [Image.fromarray(image) for image in images]\n",
        "pil_images[0]\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "H967mcvmwD_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Autoencoder"
      ],
      "metadata": {
        "id": "Wg2XIbsi74p-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "autoencoder는 image를 latent representation으로 바꾸어주는 역할을 한다."
      ],
      "metadata": {
        "id": "UvAijeVs8t70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pil_to_latent(input_im):\n",
        "  with torch.no_grad():\n",
        "    # unsqueeze(0)는 맨 앞에 차원 하나를 추가해서 배치 차원을 만든다. [3, H, W] → [1, 3, H, W]\n",
        "    # ToTensor() 의 결괏값 범위는 무조건 [0,1] 이다. vae는 근데 [-1,1] 범위로 받아야하므로 * 2 - 1을 꼭 해준다.\n",
        "    latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device) * 2 - 1)\n",
        "  return 0.18215 * latent.latent_dist.sample()\n",
        "\n",
        "def latents_to_pil(latents):\n",
        "  latents = (1 / 0.18215) * latents\n",
        "  with torch.no_grad():\n",
        "    image = vae.decode(latents).sample\n",
        "  image = (image / 2 + 0.5).clamp(0, 1)\n",
        "  image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "  images = (image * 255).round().astype(\"uint8\")\n",
        "  pil_images = [Image.fromarray(image) for image in images]\n",
        "  return pil_images"
      ],
      "metadata": {
        "id": "I9u7q3PC773z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 * 64 * 64 의 latent 이미지를 만듦으로써, 연산횟수를 매우 줄이는 효과가 있다.\n",
        "\n",
        "한 픽셀마다 los function을 적용하면서 노이즈를 제거해나가면 연산이 어마어마한데, latent space를 통해 연산을 해야할 수를 줄임으로써 효율적인 diffusion이 가능해진다."
      ],
      "metadata": {
        "id": "ry2inqQI_cvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = Image.open(\"parrot.png\").convert(\"RGB\")\n",
        "input_image = input_image.resize((512, 512))\n",
        "encoded = pil_to_latent(input_image)\n",
        "decoded = latents_to_pil(encoded)[0]"
      ],
      "metadata": {
        "id": "SNS5Wx5mEoOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Scheduler"
      ],
      "metadata": {
        "id": "wAi5z-rRAiSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diffusion을 진행하면서 매 timestep마다 노이즈를 조금씩 제거해나가야 한다.\n",
        "\n",
        "스케쥴러는 매 timestep 마다 얼마나 많은 노이즈를 제거해나가야할지 등 디테일을 담당한다."
      ],
      "metadata": {
        "id": "r-guyrkGAlgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler.set_timesteps(15)\n",
        "print(scheduler.timesteps)"
      ],
      "metadata": {
        "id": "iSyHbwOsBPUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15번에 거쳐서 진행하려면 얼마나 노이즈를 빼나가야하는 지 자동으로 계산해서 반영해준다."
      ],
      "metadata": {
        "id": "Y0-TCUqTBYlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise = torch.randn_like(encoded)\n",
        "sampling_steps = 10\n",
        "encoded_and_noised = scheduler.add_noise(encoded, noise, timesteps = torch.tensor([scheduler.timesteps[sampling_steps]]))\n",
        "latents_to_pil(encoded_and_noised.float())[0]"
      ],
      "metadata": {
        "id": "yzXErfCCDV-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "내가 입력한 이미지에 sampling step 10번을 거친만큼의 noise를 더해서\n",
        "\n",
        "latent to pil을 진행하면 노이즈가 가득 섞였지만, 조금 구도는 그대로 유지되어 보이는 이미지가 탄생한다. 이 이미지를 바탕으로 새로운 이미지를 추가하면, img to img의 기본 개념을 구현할 수 있다."
      ],
      "metadata": {
        "id": "7IJuTOl1zdYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loop Starting from noised version of input (img to img)"
      ],
      "metadata": {
        "id": "qK-wcU9z0D82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞에서 본 것과 동일하게 Diffusion을 진행하되,\n",
        "\n",
        "첫번쨰 스텝을 위에서 샘플링한 10번째 스텝으로 해서 input image와 구도는 비슷하도록 진행한다."
      ],
      "metadata": {
        "id": "dUr2yeZV0Q0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Settings (same as before except for the new prompt)\n",
        "prompt = [\"A colorful dancer, nat geo photo\"]\n",
        "height = 512                        # default height of Stable Diffusion\n",
        "width = 512                         # default width of Stable Diffusion\n",
        "num_inference_steps = 50            # Number of denoising steps\n",
        "guidance_scale = 8                  # Scale for classifier-free guidance\n",
        "generator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\n",
        "batch_size = 1\n",
        "\n",
        "# Prep text (same as before)\n",
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "# Prep Scheduler (setting the number of inference steps)\n",
        "set_timesteps(scheduler, num_inference_steps)\n",
        "\n",
        "# Prep latents (noising appropriately for start_step)\n",
        "start_step = 10\n",
        "start_sigma = scheduler.sigmas[start_step]\n",
        "noise = torch.randn_like(encoded)\n",
        "latents = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[start_step]]))\n",
        "latents = latents.to(torch_device).float()\n",
        "\n",
        "# Loop\n",
        "for i, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n",
        "    if i >= start_step: # << This is the only modification to the loop we do\n",
        "\n",
        "        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "        latent_model_input = torch.cat([latents] * 2)\n",
        "        sigma = scheduler.sigmas[i]\n",
        "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "        # predict the noise residual\n",
        "        with torch.no_grad():\n",
        "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "        # perform guidance\n",
        "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "        # compute the previous noisy sample x_t -> x_t-1\n",
        "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
        "\n",
        "latents_to_pil(latents)[0]"
      ],
      "metadata": {
        "id": "7GPoiAN20f9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "input image와 color, structure는 그대로 유지되었지만,\n",
        "프롬프트를 바탕으로 새로운 이미지가 생성되었다.\n",
        "\n",
        "이게 img2img가 작동하는 원리이다.\n",
        "\n"
      ],
      "metadata": {
        "id": "sVH_WjnB34L8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the text"
      ],
      "metadata": {
        "id": "kmQSACYIFfSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"a picture of a puppy\"\n",
        "\n",
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "text_input[\"input_ids\"][0] # >> 각 token 별 아이디\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fLMwpoaTFhoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 단어마다 tokenizer에서 지정해둔 id가 있다.\n",
        "puppy는 6829 라는 숫자로 치환된다."
      ],
      "metadata": {
        "id": "4jTeuvZrIez2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "output_embeddings"
      ],
      "metadata": {
        "id": "PQMqyV2pIqPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리는 token을 text_encoder로 pass해주고, text_encoder는 우리가 model에 넣을 수 있는 숫자값을 계산해서 전달해준다.\n",
        "\n"
      ],
      "metadata": {
        "id": "lcG5xmdEJ49Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token Embeddings"
      ],
      "metadata": {
        "id": "4z1HxT6tNb3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_emb_layer = text_encoder.text_model.embeddings.token_embedding\n",
        "token_emb_layer\n",
        "\n",
        "# 6829번째 행을 look-up해서 puppy의 emb_dim을 모두 구한다.\n",
        "embedding = token_emb_layer(torch.tensor(6829, device = torch_device))\n",
        "embedding.shape\n",
        "\n",
        "# 문장 전체 토큰 IDs를 한 번에 임베딩으로 바꾸기\n",
        "token_embeddings = token_emb_layer(text_input.input_ids.to(torch_device))\n",
        "token_embeddings"
      ],
      "metadata": {
        "id": "pHTsLRw2NgRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocab size = 49408\n",
        "\n",
        "emb_dim = 768 (한 토큰을 768개의 숫자로 표현)"
      ],
      "metadata": {
        "id": "u4I6IK7XORM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position Embeddings"
      ],
      "metadata": {
        "id": "jRcVuBLX6U43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_emb_layer = text_encoder.text_model.embeddings.position_embedding\n",
        "pos_emb_layer\n",
        "\n",
        "position_ids = text_encoder.text_model.embeddings.position_ids[:, :77]\n",
        "position_embeddings = pos_emb_layer(position_ids)\n",
        "position_embeddings"
      ],
      "metadata": {
        "id": "gKpQNXow6rCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "포지션 임베딩(position embedding)은 “토큰이 문장 안에서 몇 번째 위치에 있는지” 정보를 모델에 넣어주는 벡터"
      ],
      "metadata": {
        "id": "2aYEUdpj7R82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining Token and Text"
      ],
      "metadata": {
        "id": "FLw1K3Tj8aql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# And combining them we get the final input embeddings\n",
        "input_embeddings = token_embeddings + position_embeddings\n",
        "input_embeddings\n",
        "\n",
        "# 토큰벡터, 포지션 벡터 구하고 더하는 작업을 아래 함수가 한번에 해줌\n",
        "text_encoder.text_model.embeddings(text_input.input_ids.to(torch_device))"
      ],
      "metadata": {
        "id": "q7UzSvpm86C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 토큰의 의미와 위치를 모두 반영해야하기 때문에 두개의 임베딩을 더해주어야 한다.\n",
        "\n",
        "![transformer diagram](https://github.com/johnowhitaker/tglcourse/raw/main/images/text_encoder_noborder.png)\n",
        "\n",
        "(1) Multi-head Attention (Self-Attention)\n",
        "문장 안 토큰들이 서로를 참고하게 함\n",
        "\n",
        "예: “photo of a puppy”에서 “puppy” 토큰은 “photo”, “of”, “a” 같은 토큰들과 관계를 맺으면서 의미가 더 구체화됨.\n",
        "\n",
        "(2) Feed-Forward Network (FFN)\n",
        "\n",
        "각 토큰 벡터를 비선형 변환으로 더 “표현력 있게” 가공해주는 층.\n",
        "\n",
        "attention으로 섞인 정보를 다시 한 번 깊게 변환"
      ],
      "metadata": {
        "id": "MV7Hi9n8Dubo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 그림에서 본 TRANSFORMER BLOCK 역할\n",
        "# input embeddings에는 위에서 본 token embedding + position embedding이 들어감\n",
        "def get_output_embeds(input_embeddings):\n",
        "    bsz, seq_len = input_embeddings.shape[:2] # batch size, token 길이 꺼내기\n",
        "    # 각 토큰이 서로를 볼 수 있는 지를 제한해주는 mask\n",
        "    # causal_mask 는 \"A photo of puppy\" 문장이 있으면 앞 단어 토큰은 뒷 단어 토큰을 못보도록 제한된다.\n",
        "    causal_attention_mask = text_encoder.text_model._build_causal_attention_mask(bsz, seq_len, dtype=input_embeddings.dtype)\n",
        "\n",
        "    encoder_outputs = text_encoder.text_model.encoder(\n",
        "        inputs_embeds=input_embeddings,\n",
        "        attention_mask=None,\n",
        "        causal_attention_mask=causal_attention_mask.to(torch_device),\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=True,\n",
        "        return_dict=None,\n",
        "    )\n",
        "\n",
        "    output = encoder_outputs[0]\n",
        "\n",
        "    output = text_encoder.text_model.final_layer_norm(output)\n",
        "\n",
        "    return output\n",
        "\n",
        "out_embs_test = get_output_embeds(input_embeddings)\n",
        "print(out_embs_test.shape)\n",
        "out_embs_test"
      ],
      "metadata": {
        "id": "8I8-4FtmZht1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "지금부터 input embedding 중 puppy를 다른 embedding으로 replace 해볼 것이다."
      ],
      "metadata": {
        "id": "2w8PTgfmQ-J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"A picture of a puppy\"\n",
        "\n",
        "text_input = tokenizer(prompt, padding = \"max_length\", max_length = tokenizer.model_max_length, truncation = True, return_tensors = \"pt\" )\n",
        "input_ids = text_input.input_ids.to(torch_device)\n",
        "# token embedding 구하기\n",
        "token_embeddings = token_emb_layer(input_ids)\n",
        "# 2368 id의 token embedding vector 꺼내오기\n",
        "replacement_token_embedding = text_encoder.get_input_embeddings()(torch.tensor(2368, device = torch_device))\n",
        "# 6829 token의 인덱스를 torch.where로 찾고, 그 위치를 모두 2368로 replace\n",
        "token_embeddings[0, torch.where(input_ids[0] == 6829)] = replacement_token_embedding.to(torch.device)\n",
        "\n",
        "input_embeddings = token_embeddings + position_embeddings\n",
        "\n",
        "modified_output_embeddings = get_output_embeds(input_embeddings)\n",
        "modified_output_embeddings"
      ],
      "metadata": {
        "id": "8Bw1UeX7RLxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 이 modified embedding으로 image를 generate 해보자."
      ],
      "metadata": {
        "id": "J1VnVlkGT03Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_embs(text_embeddings):\n",
        "    height = 512\n",
        "    width = 512\n",
        "    num_inference_steps = 30\n",
        "    guidance_scale = 7.5\n",
        "    generator = torch.manual_seed(32)\n",
        "    batch_size = 1\n",
        "\n",
        "    max_length = text_input.input_ids.shape[-1]\n",
        "    uncond_input = tokenizer(\n",
        "      [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "      # token embedding + position embedding 한번에 뽑아옴\n",
        "        uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
        "    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "    # Prep Scheduler\n",
        "    set_timesteps(scheduler, num_inference_steps)\n",
        "\n",
        "    # Prep latents\n",
        "    latents = torch.randn(\n",
        "    (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "    generator=generator,\n",
        "    )\n",
        "    latents = latents.to(torch_device)\n",
        "    latents = latents * scheduler.init_noise_sigma\n",
        "\n",
        "    # Loop\n",
        "    for i, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n",
        "        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "        latent_model_input = torch.cat([latents] * 2)\n",
        "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "        # predict the noise residual\n",
        "        with torch.no_grad():\n",
        "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "        # perform guidance\n",
        "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "        # compute the previous noisy sample x_t -> x_t-1\n",
        "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
        "\n",
        "    return latents_to_pil(latents)[0]\n"
      ],
      "metadata": {
        "id": "uGONRkEGT7cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_with_embs(modified_output_embeddings)"
      ],
      "metadata": {
        "id": "_AbE1_N2Vodt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "근데 이걸 왜 했냐?\n",
        "\n",
        "token을 아예 replace 할 수 있었던 것처럼, token을 mix해서 half-puppy, half 다른거 이렇게 두가지 이미지를 합성한 이미지를 생성할 수 있다.\n",
        "\n",
        "섞을 때는 position_embedding은 섞지말고 token_embedding 끼리만 섞는다."
      ],
      "metadata": {
        "id": "Msgm0jZ_V2Vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'A picture of a puppy'\n",
        "\n",
        "# Tokenize\n",
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "input_ids = text_input.input_ids.to(torch_device)\n",
        "\n",
        "# Get token embeddings\n",
        "token_embeddings = token_emb_layer(input_ids)\n",
        "\n",
        "# token embedding으로 2가지 종의 동물을 꺼내서, 0.5씩 mix한다.\n",
        "puppy_token_embedding = token_emb_layer(torch.tensor(6829, device=torch_device))\n",
        "skunk_token_embedding = token_emb_layer(torch.tensor(42194, device=torch_device))\n",
        "replacement_token_embedding = 0.5*puppy_token_embedding + 0.5*skunk_token_embedding\n",
        "\n",
        "# Insert this into the token embeddings (\n",
        "token_embeddings[0, torch.where(input_ids[0]==6829)] = replacement_token_embedding.to(torch_device)\n",
        "\n",
        "# Combine with pos embs\n",
        "input_embeddings = token_embeddings + position_embeddings\n",
        "\n",
        "#  Feed through to get final output embs\n",
        "modified_output_embeddings = get_output_embeds(input_embeddings)\n",
        "\n",
        "# Generate an image with these\n",
        "generate_with_embs(modified_output_embeddings)"
      ],
      "metadata": {
        "id": "src6jwnIWG0H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}